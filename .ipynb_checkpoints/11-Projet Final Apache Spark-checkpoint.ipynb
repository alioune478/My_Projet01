{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Final Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nom Etudiant :**  GUEYE\n",
    "\n",
    "**Prenom Etudiant:**  ALIOUNE\n",
    "\n",
    "**Classe :**  MASTER1 BIG DATA UVS \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "Ce projet consiste à utiliser Apache Spark pour faire l'analyse et le traitement des données de **[San Francisco Fire Department Calls ](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)** afin de fournir quelques KPI (*Key Performance Indicator*). Le **SF Fire Dataset** comprend les réponses aux appels de toutes les unités d'incendie. Chaque enregistrement comprend le numéro d'appel, le numéro d'incident, l'adresse, l'identifiant de l'unité, le type d'appel et la disposition. Tous les intervalles de temps pertinents sont également inclus. Étant donné que ce Dataset est basé sur les réponses et que la plupart des appels impliquent plusieurs unités, ainsi il existe plusieurs enregistrements pour chaque numéro d'appel. Les adresses sont associées à un numéro de bloc, à une intersection ou à une boîte d'appel, et non à une adresse spécifique.\n",
    "\n",
    "**Plus de details sur la description des données cliquer sur ce [lien](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail à faire.\n",
    "L'objectif de ce projet est de comprendre le **SF Fire Dataset** afin de bien répondre aux questions en utilisant les codes Spark/Scala adéquates.\n",
    "\n",
    "- Créer un repos git (public ou privé) et partager le repos avec mon mail (limahin10@gmail.com)\n",
    "- Ecrire un code lisible et bien indenté \n",
    "- N'oublier pas de mettre en commentaire la justification de vos réponses sur les cellules Markdown. \n",
    "\n",
    "\n",
    "## Note:\n",
    "- Le projet est personnel, c'est-à-dire chaque notebook ne concerne qu'un seul étudiant. \n",
    "- Deadline : **Jeudi 10 janvier 2021** (Aucune de dérogation ne sera acceptée)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des packages Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.153:4042\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1610301380596)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import spark.implicits._\n",
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._ \n",
    "import org.apache.spark.sql.functions._ \n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@599c93da\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    appName(\"Data Processing\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@599c93da\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.rdd.compress,True)\n",
      "(spark.repl.class.outputDir,/tmp/tmpekco31tb)\n",
      "(spark.serializer.objectStreamReset,100)\n",
      "(spark.app.id,local-1610301380596)\n",
      "(spark.master,local[*])\n",
      "(spark.submit.pyFiles,)\n",
      "(spark.executor.id,driver)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.driver.port,35875)\n",
      "(spark.repl.class.uri,spark://192.168.1.153:35875/classes)\n",
      "(spark.driver.host,192.168.1.153)\n",
      "(spark.app.name,spylon-kernel)\n",
      "(spark.ui.showConsoleProgress,true)\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.getConf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons jeter un coup d'oeil sur la structure des données avant de définir un schéma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: impossible d'ouvrir 'datasets/sf-fire/sf-fire-calls.csv' en lecture: Aucun fichier ou dossier de ce type\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head -1 \"datasets/sf-fire/sf-fire-calls.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vu que la taille de ces données est énormes, inferer le schema pour un très grande volumes de données s'avère un peu couteux. Nous allons ainsi définir un schema pour le Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fireSchema: org.apache.spark.sql.types.StructType = StructType(StructField(CallNumber,IntegerType,true), StructField(UnitID,StringType,true), StructField(IncidentNumber,IntegerType,true), StructField(CallType,StringType,true), StructField(CallDate,StringType,true), StructField(WatchDate,StringType,true), StructField(CallFinalDisposition,StringType,true), StructField(AvailableDtTm,StringType,true), StructField(Address,StringType,true), StructField(City,StringType,true), StructField(Zipcode,StringType,true), StructField(Battalion,StringType,true), StructField(StationArea,StringType,true), StructField(Box,StringType,true), StructField(OriginalPriority,StringType,true), StructField(Priority,StringType,true), StructField(FinalPriority,IntegerType,true), StructField(ALSUnit,BooleanType,true),...\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fireSchema = StructType(Array(StructField(\"CallNumber\", IntegerType, true),\n",
    "  StructField(\"UnitID\", StringType, true),\n",
    "  StructField(\"IncidentNumber\", IntegerType, true),\n",
    "  StructField(\"CallType\", StringType, true),                  \n",
    "  StructField(\"CallDate\", StringType, true),      \n",
    "  StructField(\"WatchDate\", StringType, true),\n",
    "  StructField(\"CallFinalDisposition\", StringType, true),\n",
    "  StructField(\"AvailableDtTm\", StringType, true),\n",
    "  StructField(\"Address\", StringType, true),       \n",
    "  StructField(\"City\", StringType, true),       \n",
    "  StructField(\"Zipcode\", StringType, true),       \n",
    "  StructField(\"Battalion\", StringType, true),                 \n",
    "  StructField(\"StationArea\", StringType, true),       \n",
    "  StructField(\"Box\", StringType, true),       \n",
    "  StructField(\"OriginalPriority\", StringType, true),       \n",
    "  StructField(\"Priority\", StringType, true),       \n",
    "  StructField(\"FinalPriority\", IntegerType, true),       \n",
    "  StructField(\"ALSUnit\", BooleanType, true),       \n",
    "  StructField(\"CallTypeGroup\", StringType, true),\n",
    "  StructField(\"NumAlarms\", IntegerType, true),\n",
    "  StructField(\"UnitType\", StringType, true),\n",
    "  StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    "  StructField(\"FirePreventionDistrict\", StringType, true),\n",
    "  StructField(\"SupervisorDistrict\", StringType, true),\n",
    "  StructField(\"Neighborhood\", StringType, true),\n",
    "  StructField(\"Location\", StringType, true),\n",
    "  StructField(\"RowID\", StringType, true),\n",
    "  StructField(\"Delay\", FloatType, true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Path does not exist: file:/home/alioune/Téléchargements/advanced_functional_programming-20210107T133025Z-001/advanced_functional_programming/Projet_final/datasets/sf-fire/sf-fire-calls.csv;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/alioune/Téléchargements/advanced_functional_programming-20210107T133025Z-001/advanced_functional_programming/Projet_final/datasets/sf-fire/sf-fire-calls.csv;",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:764)",
      "  at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)",
      "  at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)",
      "  at scala.collection.immutable.List.flatMap(List.scala:355)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:751)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:580)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:405)",
      "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)",
      "  at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)",
      "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:723)",
      "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:553)",
      "  ... 43 elided",
      ""
     ]
    }
   ],
   "source": [
    "val sfFireFile = \"datasets/sf-fire/sf-fire-calls.csv\"\n",
    "val fireDF = spark\n",
    "  .read\n",
    "  .schema(fireSchema)\n",
    "  .option(\"header\", \"true\")\n",
    "  .csv(sfFireFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons mettre en cache le Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fireDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "36: error: not found: value fireDF",
     "output_type": "error",
     "traceback": [
      "<console>:36: error: not found: value fireDF",
      "       fireDF.count()",
      "       ^",
      ""
     ]
    }
   ],
   "source": [
    "fireDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "36: error: not found: value fireDF",
     "output_type": "error",
     "traceback": [
      "<console>:36: error: not found: value fireDF",
      "       fireDF.printSchema()",
      "       ^",
      ""
     ]
    }
   ],
   "source": [
    "fireDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "36: error: not found: value fireDF",
     "output_type": "error",
     "traceback": [
      "<console>:36: error: not found: value fireDF",
      "       fireDF.show(5)",
      "       ^",
      ""
     ]
    }
   ],
   "source": [
    "fireDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrage des d'appels de type \"Medical Incident\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "34: error: not found: value fireDF",
     "output_type": "error",
     "traceback": [
      "<console>:34: error: not found: value fireDF",
      "       val fewFireDF = fireDF",
      "                       ^",
      ""
     ]
    }
   ],
   "source": [
    "val fewFireDF = fireDF\n",
    "  .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\") \n",
    "  .where($\"CallType\" =!= \"Medical Incident\")\n",
    "\n",
    "fewFireDF.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "**Combien de types d'appels distincts ont été passés ?**  \n",
    "Pour être sûr, il ne faut pas compter les valeurs «nulles» dans la colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "37: error: not found: value fireDF",
     "output_type": "error",
     "traceback": [
      "<console>:37: error: not found: value fireDF",
      "       val  types_appels_distincts = fireDF",
      "                                     ^",
      "<console>:42: error: not found: value types_appels_distincts1",
      "       println(\"Distinct count: \"+types_appels_distincts1.count())",
      "                                  ^",
      ""
     ]
    }
   ],
   "source": [
    "// Reponse 1\n",
    "\n",
    "//Recupération de notre variable CallType \n",
    "val  types_appels_distincts = fireDF\n",
    "      .select( \"CallType\")\n",
    "      .na.drop(\"all\") \n",
    "println(\"Distinct count: \"+types_appels_distincts1.count())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quels types d'appels différents ont été passés au service d'incendie?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "36: error: not found: value types_appels_distincts",
     "output_type": "error",
     "traceback": [
      "<console>:36: error: not found: value types_appels_distincts",
      "       val types_appels_distincts1 = types_appels_distincts.distinct()",
      "                                     ^",
      ""
     ]
    }
   ],
   "source": [
    "// Reponse 2\n",
    "\n",
    "val types_appels_distincts1 = types_appels_distincts.distinct()\n",
    "      types_appels_distincts1.show(20, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "**Trouver toutes les réponses ou les délais sont supérieurs à 5 minutes?**\n",
    "\n",
    "*Indication\n",
    "1. Renommer la colonne Delay -> ReponseDelayedinMins\n",
    "2. Retourner un nouveau DataFrame\n",
    "3. Afficher tous les appels où le temps de réponse à un site d'incendie a eu lieu après un retard de plus de 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newFireDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 1) Renommer la colonne Delay -> ReponseDelayedinMins\n",
    "val newFireDF = fireDF.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------------------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|        Neighborhood|            Location|        RowID|ResponseDelayedinMins|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------------------+\n",
      "|  20110016|   T13|       2003235|  Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|2000 Block of CAL...|  SF|  94109|      B04|         38|3362|               3|       3|            3|  false|         null|        1|   TRUCK|                         2|                     4|                 5|     Pacific Heights|(37.7895840679362...|020110016-T13|                 2.95|\n",
      "|  20110022|   M17|       2003241|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 03:01:...|0 Block of SILVER...|  SF|  94124|      B10|         42|6495|               3|       3|            3|   true|         null|        1|   MEDIC|                         1|                    10|                10|Bayview Hunters P...|(37.7337623673897...|020110022-M17|                  4.7|\n",
      "|  20110023|   M41|       2003242|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 02:39:...|MARKET ST/MCALLIS...|  SF|  94102|      B03|         01|1455|               3|       3|            3|   true|         null|        1|   MEDIC|                         2|                     3|                 6|          Tenderloin|(37.7811772186856...|020110023-M41|            2.4333334|\n",
      "|  20110032|   E11|       2003250|    Vehicle Fire|01/11/2002|01/10/2002|               Other|01/11/2002 04:16:...|APPLETON AV/MISSI...|  SF|  94110|      B06|         32|5626|               3|       3|            3|  false|         null|        1|  ENGINE|                         1|                     6|                 9|      Bernal Heights|(37.7388432849018...|020110032-E11|                  1.5|\n",
      "|  20110043|   B04|       2003259|          Alarms|01/11/2002|01/10/2002|               Other|01/11/2002 06:01:...|1400 Block of SUT...|  SF|  94109|      B04|         03|3223|               3|       3|            3|  false|         null|        1|   CHIEF|                         2|                     4|                 2|    Western Addition|(37.7872890372638...|020110043-B04|            3.4833333|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//2)Retourner un nouveau DataFrame\n",
    "newFireDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+\n",
      "|CallNumber|ResponseDelayedinMins|\n",
      "+----------+---------------------+\n",
      "|  20110315|                 5.35|\n",
      "|  20120147|                 6.25|\n",
      "|  20130013|                  5.2|\n",
      "|  20140067|                  5.6|\n",
      "|  20140177|                 7.25|\n",
      "|  20150056|            11.916667|\n",
      "|  20150254|             5.116667|\n",
      "|  20150265|             8.633333|\n",
      "|  20150265|             95.28333|\n",
      "|  20150380|                 5.45|\n",
      "+----------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Delay_Sup_Cinq: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallNumber: int, UnitID: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//3)Afficher tous les appels où le temps de réponse à un site d'incendie a eu lieu après un retard de plus de 5 minutes\n",
    "//Delay_Sup_Cinq.show()\n",
    "val Delay_Sup_Cinq=newFireDF.where($\"ResponseDelayedinMins\" > 5)\n",
    "Delay_Sup_Cinq.select(\"CallNumber\",\"ResponseDelayedinMins\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations des dates\n",
    "Maintenant nous allons d'abord:\n",
    "1. Transformer les dates de type String en Spark Timestamp afin que nous puissions effectuer des requêtes basées sur la date plus tard\n",
    "2. Retourner le Dataframe transformée\n",
    "3. Mettre en cache le nouveau DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fireTSDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\n",
       "res85: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallNumber: int, UnitID: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var fireTSDF = newFireDF\n",
    "  .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\") \n",
    "  .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\")).drop(\"WatchDate\") \n",
    "  .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy hh:mm:ss a\")).drop(\"AvailableDtTm\")\n",
    "\n",
    "fireTSDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "fireTSDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res87: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallNumber: int, UnitID: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fireTSDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res88: org.apache.spark.sql.DataFrame = [Zipcode: string]\n"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " fireTSDF.select(\"Zipcode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "**Quels sont les types d'appels les plus courants?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|         CallType| count|\n",
      "+-----------------+------+\n",
      "| Medical Incident|113794|\n",
      "|   Structure Fire| 23319|\n",
      "|           Alarms| 19406|\n",
      "|Traffic Collision|  7013|\n",
      "+-----------------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "typ_appels_plus_courants: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallType: string, count: bigint]\n"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Reponse 4\n",
    "/*\n",
    "Ecrire ici votre code\n",
    "*/\n",
    "val  typ_appels_plus_courants = fireTSDF\n",
    "\n",
    "  .groupBy(\"CallType\").count()\n",
    "  .orderBy($\"count\".desc)\n",
    "typ_appels_plus_courants.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5-a\n",
    "**Quels sont boites postaux rencontrés dans les appels les plus courants?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|         CallType| count|\n",
      "+-----------------+------+\n",
      "| Medical Incident|113794|\n",
      "|   Structure Fire| 23319|\n",
      "|           Alarms| 19406|\n",
      "|Traffic Collision|  7013|\n",
      "+-----------------+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "typ_appels_plus_courants: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallType: string, count: bigint]\n"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val  typ_appels_plus_courants = fireTSDF\n",
    "\n",
    "  .groupBy(\"CallType\").count()\n",
    "  .orderBy($\"count\".desc)\n",
    "typ_appels_plus_courants.show(4)\n",
    "\n",
    "typ_appels_plus_courants.select\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "//fireTSDF01.select(\"Zipcode_CallType\").show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Reponse 5-a\n",
    "/*\n",
    "Ecrire ici votre code\n",
    "*/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5-b\n",
    "**Quels sont les quartiers de San Francisco dont les codes postaux sont 94102 et 94103?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|City|Zipcode|\n",
      "+----+-------+\n",
      "|  SF|  94102|\n",
      "|  SF|  94102|\n",
      "|  SF|  94103|\n",
      "|  SF|  94103|\n",
      "|  SF|  94103|\n",
      "|  SF|  94103|\n",
      "|  SF|  94102|\n",
      "|  SF|  94102|\n",
      "|  SF|  94102|\n",
      "|  SF|  94102|\n",
      "|  SF|  94103|\n",
      "|  SF|  94102|\n",
      "|  SF|  94103|\n",
      "|  SF|  94102|\n",
      "|  SF|  94103|\n",
      "|  SF|  94102|\n",
      "|  SF|  94102|\n",
      "|  SF|  94102|\n",
      "|  SF|  94103|\n",
      "|  SF|  94103|\n",
      "+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "filtre1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallNumber: int, UnitID: string ... 26 more fields]\n",
       "quartiers_San_Francisco: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [CallNumber: int, UnitID: string ... 26 more fields]\n"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Reponse 5-b\n",
    "\n",
    "val filtre1 = fireTSDF\n",
    "     .filter(fireTSDF(\"Zipcode\") === \"94102\" ||fireTSDF(\"Zipcode\") === \"94103\")\n",
    "\n",
    "val quartiers_San_Francisco = filtre1 \n",
    "     .filter(filtre1(\"City\") === \"SF\")\n",
    "    \n",
    "\n",
    "quartiers_San_Francisco.select(\"City\",\"Zipcode\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "**Determiner le nombre total d'appels, ainsi que la moyenne, le minimum et le maximum du temps de réponse des appels?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------+\n",
      "|summary|responseDelayedinMins|\n",
      "+-------+---------------------+\n",
      "|  count|               175296|\n",
      "|   mean|    3.892364154521585|\n",
      "| stddev|    9.378286226254206|\n",
      "|    min|          0.016666668|\n",
      "|    max|              1844.55|\n",
      "+-------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Reponse 6 Le nombre total d'appels ainsi que  le minimum et le maximum du temps de réponse des appels:\n",
    "fireTSDF.select(\"responseDelayedinMins\")\n",
    "        .describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7-a\n",
    "**Combien d'années distinctes trouve t-on dans ce Dataset?**  \n",
    "Dans ce dataset nous avons des données comprises entre 2000-2018. Vous pouvez utilisez la fonction Spark `year()` pour les dates en Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le nombre d'années distinctes: 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nombre_années_distinctes: org.apache.spark.sql.DataFrame = [year(IncidentDate): int]\n"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Reponse 7-a\n",
    "\n",
    "val nombre_années_distinctes = fireTSDF.select(year(fireTSDF(\"IncidentDate\")))\n",
    "                                .distinct()\n",
    "                                .na.drop(\"all\") \n",
    "println(\"le nombre d'années distinctes: \"+nombre_années_distinctes.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res114: Array[String] = Array(CallNumber, UnitID, IncidentNumber, CallType, CallFinalDisposition, Address, City, Zipcode, Battalion, StationArea, Box, OriginalPriority, Priority, FinalPriority, ALSUnit, CallTypeGroup, NumAlarms, UnitType, UnitSequenceInCallDispatch, FirePreventionDistrict, SupervisorDistrict, Neighborhood, Location, RowID, ResponseDelayedinMins, IncidentDate, OnWatchDate, AvailableDtTS)\n"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fireTSDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7-b\n",
    "**Quelle semaine de l'année 2018 a eu le plus d'appels d'incendie?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Reponse 7-b\n",
    "/*\n",
    "Ecrire ici votre code\n",
    "*/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 27 more fields]\n",
       "res134: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 28 more fields]\n"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// création de la colonne week_of_year\n",
    "val df1= fireTSDF.withColumn(\"IncidentDate\",\n",
    "    to_timestamp(col(\"IncidentDate\")))\n",
    "    .withColumn(\"year\", date_format(col(\"IncidentDate\"), \"y\"))\n",
    "\n",
    "// création de la colonne week_of_year\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.withColumn(\"IncidentDate\",\n",
    "    to_timestamp(col(\"IncidentDate\")))\n",
    "    .withColumn(\"day-of-year\", date_format(col(\"IncidentDate\"), \"D\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res140: Array[String] = Array(CallNumber, UnitID, IncidentNumber, CallType, CallFinalDisposition, Address, City, Zipcode, Battalion, StationArea, Box, OriginalPriority, Priority, FinalPriority, ALSUnit, CallTypeGroup, NumAlarms, UnitType, UnitSequenceInCallDispatch, FirePreventionDistrict, SupervisorDistrict, Neighborhood, Location, RowID, ResponseDelayedinMins, IncidentDate, OnWatchDate, AvailableDtTS, year)\n"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2003|\n",
      "|              2007|\n",
      "|              2018|\n",
      "|              2015|\n",
      "|              2006|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2004|\n",
      "|              2012|\n",
      "|              2009|\n",
      "|              2016|\n",
      "|              2001|\n",
      "|              2005|\n",
      "|              2000|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2008|\n",
      "|              2017|\n",
      "|              2002|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nombre_années_distinctes.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "**Quels sont les quartiers de San Francisco qui ont connu le pire temps de réponse en 2018?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---------------------+\n",
      "|             Address|year|responseDelayedinMins|\n",
      "+--------------------+----+---------------------+\n",
      "|600 Block of UNIO...|2018|            491.26666|\n",
      "|200 Block of MARK...|2018|            406.63333|\n",
      "|300 Block of EDDY ST|2018|            340.48334|\n",
      "|100 Block of CARL ST|2018|            175.86667|\n",
      "|200 Block of WILL...|2018|                155.8|\n",
      "|200 Block of THE ...|2018|            135.51666|\n",
      "|1900 Block of CAL...|2018|            129.01666|\n",
      "|1100 Block of 22N...|2018|                109.8|\n",
      "|1500 Block of 7TH...|2018|            106.13333|\n",
      "|500 Block of MINN...|2018|             94.71667|\n",
      "|2400 Block of KEI...|2018|            92.816666|\n",
      "|500 Block of MINN...|2018|            91.666664|\n",
      "|400 Block of 6TH AVE|2018|            90.433334|\n",
      "|500 Block of CART...|2018|             83.76667|\n",
      "| 200 Block of 6TH ST|2018|                 76.9|\n",
      "|1000 Block of POL...|2018|            76.566666|\n",
      "|4000 Block of 18T...|2018|             74.13333|\n",
      "|1200 Block of WEB...|2018|            67.916664|\n",
      "|1100 Block of POS...|2018|                67.45|\n",
      "| 200 Block of 8TH ST|2018|            64.683334|\n",
      "+--------------------+----+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "quartiers_San_Francisco_pire: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Address: string, year: string ... 1 more field]\n"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Reponse 8\n",
    "/*\n",
    "Ecrire ici votre code\n",
    "*/\n",
    "val quartiers_San_Francisco_pire = df1 \n",
    "     .filter(df1(\"year\") === 2018)\n",
    "    .select(\"Address\",\"year\",\"responseDelayedinMins\")\n",
    "    .orderBy($\"responseDelayedinMins\".desc)\n",
    "\n",
    "quartiers_San_Francisco_pire.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "**Comment stocker les données du Dataframe sous format de fichiers Parquet?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Reponse 9\n",
    "/*\n",
    "Ecrire ici votre code\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "**Comment relire les données stockée en format Parquet?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Reponse 10\n",
    "/*\n",
    "Ecrire ici votre code\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
